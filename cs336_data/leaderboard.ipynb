{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daed43e0",
   "metadata": {},
   "source": [
    "### 1. Download data\n",
    "#### 1.1 CC WET files\n",
    "- since I do not have access, I will just download 5k `.warc.wet.gz` to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad1eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-18/wet.paths.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1089752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from xopen import xopen\n",
    "\n",
    "base_url = \"https://data.commoncrawl.org/\"\n",
    "MOUNT_DIR = Path(\"/home/azureuser/mount/\")\n",
    "N_CPU = len(os.sched_getaffinity(0))\n",
    "N_WET = 100\n",
    "\n",
    "def download_file(url, output_dir):\n",
    "    filename = Path(url).name\n",
    "    output_path = output_dir / filename\n",
    "    \n",
    "    if output_path.exists():\n",
    "        return True, f\"Skipped: {filename}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return True, f\"Downloaded: {filename}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Error {filename}: {e}\"\n",
    "\n",
    "# Read all paths\n",
    "with xopen('wet.paths.gz', 'rt') as f:\n",
    "    all_paths = [line.strip() for line in f]\n",
    "\n",
    "output_dir = Path(MOUNT_DIR/\"CC\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Download until we have N_WET successful downloads\n",
    "successful_downloads = 0\n",
    "path_idx = 0\n",
    "futures = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=N_CPU) as executor:\n",
    "    # Submit initial batch\n",
    "    while path_idx < len(all_paths) and len(futures) < N_CPU:\n",
    "        url = base_url + all_paths[path_idx]\n",
    "        future = executor.submit(download_file, url, output_dir)\n",
    "        futures[future] = path_idx\n",
    "        path_idx += 1\n",
    "    \n",
    "    # Process results and submit new jobs as needed\n",
    "    while successful_downloads < N_WET and futures:\n",
    "        done, _ = as_completed(futures), None\n",
    "        \n",
    "        for future in list(futures.keys()):\n",
    "            if future.done():\n",
    "                success, message = future.result()\n",
    "                print(message)\n",
    "                \n",
    "                if success and not message.startswith(\"Skipped\"):\n",
    "                    successful_downloads += 1\n",
    "                \n",
    "                del futures[future]\n",
    "                \n",
    "                # Submit new job if we need more downloads\n",
    "                if successful_downloads < N_WET and path_idx < len(all_paths):\n",
    "                    url = base_url + all_paths[path_idx]\n",
    "                    new_future = executor.submit(download_file, url, output_dir)\n",
    "                    futures[new_future] = path_idx\n",
    "                    path_idx += 1\n",
    "                \n",
    "                break\n",
    "\n",
    "print(f\"\\nCompleted: {successful_downloads} successful downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf645c",
   "metadata": {},
   "source": [
    "#### 1.2 validation data - paloma c4_100_domains - val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2588070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/localfiles/cs336-assignment4-data-mine/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14059\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(token=\"\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "paloma_c4_100_domains_val = load_dataset(\"allenai/paloma\", \"c4_100_domains\", split=\"val\")\n",
    "print(len(paloma_c4_100_domains_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a86e7",
   "metadata": {},
   "source": [
    "### 2. Processing\n",
    "- TLD (top-level domain) filtering\n",
    "    - checked TLD from `paloma` ds and they are quite normal\n",
    "- Quality rules (number of words, lengths of words, etc.)\n",
    "- The validation dataset looks all English, so I will keep only English data\n",
    "    - To determine the threshold, I ran the model on `paloma` and get the average of `0.95`. Thus I will use `0.9` for filtering to be on the safe side, and do further removal if needed.\n",
    "- Harmful removal. \n",
    "    - To determine the threshold, I ran the model on `paloma` and get the average of `0.99`. Thus I will use `0.9` for filtering to be on the safe side, and do further removal if needed.\n",
    "- Deduplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL filtering strategies using tldextract\n",
    "from tldextract import TLDExtract\n",
    "def should_filter_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if URL should be filtered out (removed)\n",
    "    \"\"\"\n",
    "    # Create the extractor instance\n",
    "    extractor = TLDExtract()\n",
    "    if not url:\n",
    "        return True\n",
    "    try:\n",
    "        extracted = extractor(url)  # Use the class instance\n",
    "        domain = extracted.domain.lower()\n",
    "        suffix = extracted.suffix.lower()\n",
    "        \n",
    "        # based on exploration of paloma dataset\n",
    "        # it has no intersection with `adult` and `social` domains\n",
    "        allowed_tlds = {'com', 'org', 'edu', 'gov', 'net', 'uk', 'ca', 'au', 'us'}\n",
    "        adult_domains = {\n",
    "            'pornhub', 'xvideos', 'redtube', 'youporn', 'xhamster',\n",
    "            'tube8', 'spankbang', 'chaturbate', 'cam4', 'livejasmin'\n",
    "        }\n",
    "        social_domains = {\n",
    "            'facebook', 'twitter', 'instagram', 'tiktok', 'snapchat',\n",
    "            'reddit', '4chan', '8chan', 'discord', 'telegram'\n",
    "        }\n",
    "\n",
    "        # 1. Filter by top-level domain (keep only certain TLDs)\n",
    "        if suffix not in allowed_tlds:\n",
    "            return True\n",
    "        # 2. Filter out adult/inappropriate domains\n",
    "        if domain in adult_domains:\n",
    "            return True\n",
    "        # 3. Filter out social media/forum content (often low quality)\n",
    "        if domain in social_domains:\n",
    "            return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        # Filter out URLs that can't be parsed\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1657fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_data.language_identification import identify_language\n",
    "from cs336_data.harmful_content import classify_nsfw, classify_toxic_speech\n",
    "\n",
    "subset = paloma_c4_100_domains_val.shuffle(seed=42).select(range(1000))  # Random 1000 examples\n",
    "\n",
    "# Option 1: Using .map() - most efficient for datasets\n",
    "# This applies the function to each item and returns a new dataset with the results\n",
    "def process_language(example):\n",
    "    lang, score = identify_language(example[\"text\"])  # Call once, unpack results\n",
    "    return {\"lang\": lang, \"score\": score}\n",
    "\n",
    "lang_results = subset.map(\n",
    "    process_language,\n",
    "    num_proc=16,  # Use multiple processes for speed\n",
    "    batch_size=100,  # Process in batches\n",
    "    load_from_cache_file=True  # Cache results\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "print(np.mean(lang_results['score']))\n",
    "print(np.std(lang_results['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd54704",
   "metadata": {},
   "outputs": [],
   "source": [
    "wet_file = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastwarc.warc import ArchiveIterator, WarcRecordType\n",
    "from resiliparse.parse.encoding import detect_encoding\n",
    "from cs336_data.gopher_quality_filter import gopher_quality_filter\n",
    "\n",
    "SCORE_LANG = 0.90\n",
    "SCORE_NSFW = 0.90\n",
    "SCORE_TOXIC = 0.90\n",
    "\n",
    "def process_single_wet_file(input_path: str, output_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a single WET file with language, toxicity, and NSFW filtering.\n",
    "    Returns summary statistics.\n",
    "    \"\"\"\n",
    "    iterator = ArchiveIterator(open(input_path, \"rb\"))\n",
    "    for record in iterator:\n",
    "        # 0. check record type\n",
    "        if record.record_type != WarcRecordType.conversion:\n",
    "            continue\n",
    "        byte_string = record.reader.read()\n",
    "        encoding = detect_encoding(byte_string)\n",
    "        content = byte_string.decode(encoding, errors=\"ignore\")\n",
    "\n",
    "        # 1. check URL\n",
    "        if should_filter_url(record.headers.get('WARC-Target-URI', '')):\n",
    "            continue\n",
    "\n",
    "        # 2. Quality check, rule-based\n",
    "        if not gopher_quality_filter(content):\n",
    "            continue\n",
    "\n",
    "        # 3. Identify language\n",
    "        lang, score = identify_language(content)\n",
    "        if lang != \"en\" or score < SCORE_LANG:\n",
    "            continue\n",
    "\n",
    "        # 4. Classify NSFW\n",
    "        nsfw, score = identify_language(content)\n",
    "        if not nsfw.startswith(\"non-\") or score < SCORE_NSFW:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89a56929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.record_type == WarcRecordType.conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d29962c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('WARC-Type', 'conversion'), ('WARC-Target-URI', 'http://96-hi.com/index.phtml?PUT=gift_send&AID=207437&FID=1381841'), ('WARC-Date', '2025-04-17T14:47:56Z'), ('WARC-Record-ID', '<urn:uuid:ade66f60-c05b-4e80-8020-fed0882340e0>'), ('WARC-Refers-To', '<urn:uuid:191653f5-aefc-4a64-b8d1-e6e78c8ce753>'), ('WARC-Block-Digest', 'sha1:IHBR2RSEACFSWCDACZOHOAZZCSY6PNMX'), ('WARC-Identified-Content-Language', 'zho,eng'), ('Content-Type', 'text/plain'), ('Content-Length', '1413'))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea27504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a05c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6abe94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_BAR = 0.90\n",
    "NSFW_BAR = 0.90\n",
    "TOXIC_BAR = 0.90\n",
    "\n",
    "def process_single_wet_file(input_path: str, output_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a single WET file with language, toxicity, and NSFW filtering.\n",
    "    Returns summary statistics.\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    from fastwarc.warc import ArchiveIterator, WarcRecordType\n",
    "    \n",
    "    stats = {\n",
    "        'total_records': 0,\n",
    "        'conversion_records': 0,\n",
    "        'passed_language': 0,\n",
    "        'passed_nsfw': 0,\n",
    "        'passed_toxic': 0,\n",
    "        'final_kept': 0,\n",
    "        'total_chars_kept': 0\n",
    "    }\n",
    "    \n",
    "    with gzip.open(input_path, 'rb') as input_file, \\\n",
    "         gzip.open(output_path, 'wt', encoding='utf-8') as output_file:\n",
    "        \n",
    "        for record in ArchiveIterator(input_file):\n",
    "            stats['total_records'] += 1\n",
    "            \n",
    "            # Only process conversion records (WET content)\n",
    "            if record.record_type == WarcRecordType.conversion:\n",
    "                stats['conversion_records'] += 1\n",
    "                \n",
    "                try:\n",
    "                    # Extract text content\n",
    "                    text = record.reader().read().decode('utf-8', errors='ignore')\n",
    "                    \n",
    "                    # Skip very short texts\n",
    "                    if len(text.strip()) < 100:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get URL from headers\n",
    "                    url = record.headers.get('WARC-Target-URI', '')\n",
    "                    \n",
    "                    # URL filtering - skip low-quality domains\n",
    "                    if should_filter_url(url):\n",
    "                        continue\n",
    "                    \n",
    "                    # Language identification\n",
    "                    lang, lang_score = identify_language(text)\n",
    "                    if lang != 'en' or lang_score < LANG_BAR:\n",
    "                        continue\n",
    "                    stats['passed_language'] += 1\n",
    "                    \n",
    "                    # NSFW classification\n",
    "                    nsfw_score = classify_nsfw(text)\n",
    "                    if nsfw_score > NSFW_BAR:\n",
    "                        continue\n",
    "                    stats['passed_nsfw'] += 1\n",
    "                    \n",
    "                    # Toxicity classification\n",
    "                    toxic_score = classify_toxic_speech(text)\n",
    "                    if toxic_score > TOXIC_BAR:\n",
    "                        continue\n",
    "                    stats['passed_toxic'] += 1\n",
    "                    \n",
    "                    # All filters passed - write to output\n",
    "                    output_file.write(text + '\\n\\n')\n",
    "                    stats['final_kept'] += 1\n",
    "                    stats['total_chars_kept'] += len(text)\n",
    "                    \n",
    "                    # Progress indicator\n",
    "                    if stats['final_kept'] % 100 == 0:\n",
    "                        print(f\"Kept {stats['final_kept']} documents so far...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Skip problematic records\n",
    "                    print(f\"Error processing record: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Return summary\n",
    "    summary = f\"\"\"\n",
    "Processing complete for {input_path}:\n",
    "- Total records: {stats['total_records']}\n",
    "- Conversion records: {stats['conversion_records']}\n",
    "- Passed language filter: {stats['passed_language']}\n",
    "- Passed NSFW filter: {stats['passed_nsfw']}\n",
    "- Passed toxicity filter: {stats['passed_toxic']}\n",
    "- Final documents kept: {stats['final_kept']}\n",
    "- Total characters kept: {stats['total_chars_kept']:,}\n",
    "- Filter rates: Lang={stats['passed_language']/max(stats['conversion_records'],1):.2%}, NSFW={stats['passed_nsfw']/max(stats['passed_language'],1):.2%}, Toxic={stats['passed_toxic']/max(stats['passed_nsfw'],1):.2%}\n",
    "\"\"\"\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b4d0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastwarc.warc import ArchiveIterator, WarcRecordType\n",
    "from tldextract import TLDExtract\n",
    "\n",
    "wet_file = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz\"\n",
    "\n",
    "iterator = ArchiveIterator(open(wet_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "40263a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WarcRecordType.conversion: 128>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = next(iterator)\n",
    "record.record_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae9dd075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WarcRecordType.conversion: 128>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WarcRecordType.conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function on a single file\n",
    "test_input = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data/CC-MAIN-20250417135010-20250417165010-00065.warc.wet.gz\"\n",
    "test_output = \"/tmp/test_filtered.txt.gz\"\n",
    "\n",
    "# Run the processing function\n",
    "result = process_single_wet_file(test_input, test_output)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9564bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all downloaded WET files in parallel\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "\n",
    "def process_all_wet_files(wet_dir: str, output_dir: str, max_workers: int = N_CPU):\n",
    "    \"\"\"Process all WET files in parallel\"\"\"\n",
    "    \n",
    "    # Find all WET files\n",
    "    wet_files = glob.glob(f\"{wet_dir}/*.warc.wet.gz\")\n",
    "    print(f\"Found {len(wet_files)} WET files to process\")\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Process files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all jobs\n",
    "        futures = {}\n",
    "        for wet_file in wet_files:\n",
    "            filename = Path(wet_file).stem  # Remove .gz extension\n",
    "            output_file = Path(output_dir) / f\"{filename}_filtered.txt.gz\"\n",
    "            \n",
    "            future = executor.submit(process_single_wet_file, wet_file, str(output_file))\n",
    "            futures[future] = wet_file\n",
    "        \n",
    "        # Collect results\n",
    "        for future in as_completed(futures):\n",
    "            wet_file = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(f\"✓ Completed: {Path(wet_file).name}\")\n",
    "                print(result)\n",
    "                print(\"-\" * 80)\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed: {Path(wet_file).name} - {e}\")\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# wet_directory = \"/home/azureuser/mount/CC\"  # Where your downloaded files are\n",
    "# output_directory = \"/home/azureuser/mount/CC_filtered\"  # Where to save filtered files\n",
    "# process_all_wet_files(wet_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4986cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL filtering strategies using tldextract\n",
    "from tldextract import TLDExtract\n",
    "\n",
    "# Create the extractor instance\n",
    "extractor = TLDExtract()\n",
    "\n",
    "def should_filter_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if URL should be filtered out (removed)\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        extracted = extractor(url)  # Use the class instance\n",
    "        domain = extracted.domain.lower()\n",
    "        suffix = extracted.suffix.lower()\n",
    "        subdomain = extracted.subdomain.lower()\n",
    "        \n",
    "        # 1. Filter by top-level domain (keep only certain TLDs)\n",
    "        allowed_tlds = {'com', 'org', 'edu', 'gov', 'net', 'co.uk', 'ca', 'au'}\n",
    "        if suffix not in allowed_tlds:\n",
    "            return True\n",
    "        \n",
    "        # 2. Filter out known low-quality or spam domains\n",
    "        spam_domains = {\n",
    "            'blogspot', 'wordpress', 'tumblr', 'livejournal', 'geocities',\n",
    "            'angelfire', 'weebly', 'wix', 'squarespace', 'medium'  # Personal blogs\n",
    "        }\n",
    "        if domain in spam_domains:\n",
    "            return True\n",
    "        \n",
    "        # 3. Filter out adult/inappropriate domains\n",
    "        adult_domains = {\n",
    "            'pornhub', 'xvideos', 'redtube', 'youporn', 'xhamster',\n",
    "            'tube8', 'spankbang', 'chaturbate', 'cam4', 'livejasmin'\n",
    "        }\n",
    "        if domain in adult_domains:\n",
    "            return True\n",
    "        \n",
    "        # 4. Filter out social media/forum content (often low quality)\n",
    "        social_domains = {\n",
    "            'facebook', 'twitter', 'instagram', 'tiktok', 'snapchat',\n",
    "            'reddit', '4chan', '8chan', 'discord', 'telegram'\n",
    "        }\n",
    "        if domain in social_domains:\n",
    "            return True\n",
    "        \n",
    "        # 5. Filter out e-commerce/shopping sites (product descriptions)\n",
    "        ecommerce_domains = {\n",
    "            'amazon', 'ebay', 'alibaba', 'aliexpress', 'etsy',\n",
    "            'shopify', 'walmart', 'target', 'bestbuy'\n",
    "        }\n",
    "        if domain in ecommerce_domains:\n",
    "            return True\n",
    "        \n",
    "        # 6. Filter suspicious subdomains\n",
    "        suspicious_subdomains = {\n",
    "            'ads', 'ad', 'advertisement', 'promo', 'spam',\n",
    "            'affiliate', 'click', 'tracker', 'analytics'\n",
    "        }\n",
    "        if subdomain in suspicious_subdomains:\n",
    "            return True\n",
    "        \n",
    "        # 7. Prefer high-quality domains (news, education, government)\n",
    "        high_quality_domains = {\n",
    "            'wikipedia', 'bbc', 'cnn', 'reuters', 'nytimes',\n",
    "            'mit', 'stanford', 'harvard', 'cambridge', 'oxford',\n",
    "            'nasa', 'nih', 'cdc', 'who', 'unesco'\n",
    "        }\n",
    "        # You could prioritize these instead of filtering\n",
    "        \n",
    "        # 8. Filter by URL patterns\n",
    "        url_lower = url.lower()\n",
    "        if any(pattern in url_lower for pattern in [\n",
    "            '/ads/', '/advertisement/', '/promo/', '/affiliate/',\n",
    "            'utm_source', 'utm_medium', 'utm_campaign',  # Tracking URLs\n",
    "            '?ref=', '&ref=', 'referrer=',\n",
    "            '/category/', '/tag/', '/search?'  # Navigation pages\n",
    "        ]):\n",
    "            return True\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    except Exception:\n",
    "        # Filter out URLs that can't be parsed\n",
    "        return True\n",
    "\n",
    "# Example usage in your processing function:\n",
    "def process_single_wet_file_with_url_filter(input_path: str, output_path: str) -> str:\n",
    "    \"\"\"Enhanced version with URL filtering\"\"\"\n",
    "    # ... (previous code) ...\n",
    "    \n",
    "    # Add URL filtering after extracting URL but before other filters:\n",
    "    # url = record.headers.get('WARC-Target-URI', '')\n",
    "    # if should_filter_url(url):\n",
    "    #     continue\n",
    "    \n",
    "    pass\n",
    "\n",
    "print(\"URL filtering functions defined!\")\n",
    "print(\"Example:\")\n",
    "test_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Machine_Learning\",\n",
    "    \"https://myblog.blogspot.com/2023/random-post\",\n",
    "    \"https://www.amazon.com/product/12345\",\n",
    "    \"https://ads.google.com/click?id=123\"\n",
    "]\n",
    "\n",
    "for url in test_urls:\n",
    "    print(f\"{url}: {'FILTER' if should_filter_url(url) else 'KEEP'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
