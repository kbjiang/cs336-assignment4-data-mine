{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ca71e1",
   "metadata": {},
   "source": [
    "## Parse and extract text from `warc` file\n",
    "1. With `fastwarc` and `resiliparse`\n",
    "    1. https://resiliparse.chatnoir.eu/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastwarc import ArchiveIterator\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from resiliparse.parse.encoding import detect_encoding\n",
    "\n",
    "def extract_text(record):\n",
    "    byte_string = record.reader.read()\n",
    "    encoding = detect_encoding(byte_string)\n",
    "    html_content = byte_string.decode(encoding=encoding)\n",
    "    extracted_text = extract_plain_text(html_content)\n",
    "    return extracted_text\n",
    "\n",
    "warc_file = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\"\n",
    "iterator = ArchiveIterator(open(warc_file, \"rb\"), func_filter=lambda r: r.headers.get('WARC-Identified-Payload-Type') == 'text/html')\n",
    "\n",
    "# record = next(iterator)\n",
    "# print(extract_text(record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebca25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get random records for ensuing tests\n",
    "# Method: Skip randomly through iterator\n",
    "import random\n",
    "\n",
    "def get_random_records(iterator, N=20, skip_prob=0.9):\n",
    "    \"\"\"Skip records randomly and return extracted text immediately\"\"\"\n",
    "    random_data = []\n",
    "    for i, record in enumerate(iterator):\n",
    "        if len(random_data) >= N:\n",
    "            break\n",
    "        if random.random() > skip_prob:\n",
    "            # Read and process immediately while record is still fresh\n",
    "            extracted_text = extract_text(record)\n",
    "            random_data.append((i, extracted_text))\n",
    "    return random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f449b6",
   "metadata": {},
   "source": [
    "## Language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"lid.176.bin\")\n",
    "\n",
    "# sanity check\n",
    "model.predict(\"Hello world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe62814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_generator():\n",
    "    for i, text in random_data:\n",
    "        print(f\"=== Record {i} ===\")\n",
    "        print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "        lang, score = model.predict(text.replace(\"\\n\", \" \"))\n",
    "        print(f\"Language: {lang[0]}, Score: {score[0]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        yield \n",
    "\n",
    "# Get random records with extracted text\n",
    "random_data = get_random_records(iterator)\n",
    "\n",
    "# Create the generator\n",
    "lang_gen = language_generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dbf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell multiple times to get one result at a time\n",
    "next(lang_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4f272",
   "metadata": {},
   "source": [
    "## PII\n",
    "### email address\n",
    "- Length: The total length of an email address is capped at 320 characters, with 64 for the username and 255 for the domain.\n",
    "- Spaces: Spaces are not allowed.\n",
    "- Case sensitivity: Email addresses are generally not case-sensitive, meaning User@Example.com is the same as user@example.com.\n",
    "\n",
    "- Special characters:\n",
    "    - Periods (.), hyphens (-), and underscores (_) are often allowed in the local part.\n",
    "    - They cannot be the first or last character of the local part and cannot appear consecutively (e.g., john..doe@example.com is invalid).\n",
    "    - In the domain, hyphens are allowed but not at the beginning or end of a label (a part between periods). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3161958",
   "metadata": {},
   "source": [
    "### US phone number\n",
    "1. use pattern\n",
    "    - (\\+1\\s*)? - optional `+1` followed by optional spaces\n",
    "    - \\(? - optional opening parenthesis\n",
    "    - \\d{3} - 3 digits\n",
    "    - \\)? - optional closing parenthesis\n",
    "    - [\\s-]? - at most one space or hyphen (the ? means zero or one)\n",
    "    - \\s* - zero or more additional spaces\n",
    "    - \\d{3} - 3 digits\n",
    "    - [\\s-]? - at most one space or hyphen\n",
    "    - \\s* - zero or more additional spaces\n",
    "    - \\d{4} - 4 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3691ca",
   "metadata": {},
   "source": [
    "### IP address\n",
    "- Use `\\b` to avoid matching things like:\n",
    "    - 1.2.3.4.5 (too many octets)\n",
    "    - version1.2.3.4 (prefix attached)\n",
    "    - 1.2.3.4th (suffix attached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a8216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask_pii import mask_email, mask_phone, mask_ip\n",
    "\n",
    "record = next(iterator)\n",
    "text = extract_text(record)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, mask_email_counts = mask_email(text)\n",
    "text, mask_phone_counts = mask_phone(text)\n",
    "text, mask_ip_counts = mask_ip(text)\n",
    "\n",
    "if any([counts > 0 for counts in [mask_email_counts, mask_phone_counts, mask_ip_counts]]):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d571a",
   "metadata": {},
   "source": [
    "## Harmful content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe55101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(\"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data/jigsaw_fasttext_bigrams_nsfw_final.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"SUCK MY C*CK WIKIPEDIA EDITORS...F*CKING *SSH*LE DORKS. \"\n",
    "    \"JUST TRYING TO MAKE THE SITE BETTER YOU UPTIGHT C*NTS\"\n",
    ")\n",
    "# text = \"Umm, theres no actual article for prostitution ring.  - Crunch Captain.\"\n",
    "text = \"操你妈 你个傻逼 干死你\"\n",
    "\n",
    "model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df020d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from harmful_content import classify_nsfw, classify_toxic_speech\n",
    "\n",
    "found = False\n",
    "while found == False:\n",
    "    record = next(iterator)\n",
    "    try:\n",
    "        text = extract_text(record)\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    # print(text)\n",
    "    is_nsfw, score_nsfw = classify_nsfw(text)\n",
    "    is_toxic, score_toxic = classify_toxic_speech(text)\n",
    "\n",
    "    if is_nsfw == \"nsfw\" or is_toxic == \"toxic\":\n",
    "        print(text)\n",
    "        print(\"=\"*80)\n",
    "        print(\"Harmful content detected!\")\n",
    "        print(is_nsfw, is_toxic)\n",
    "        print(score_nsfw, score_toxic)\n",
    "\n",
    "        found = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc12f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b05ca5b",
   "metadata": {},
   "source": [
    "## Quality Rule\n",
    "- Contain less than 50 or more than 100,000 words.\n",
    "- Have a mean word length outside the range of 3 to 10 characters.\n",
    "- Have more than 30% of lines ending with an ellipsis (“...”).\n",
    "- Contain less than 80% of words with at least one alphabetic character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def valid_words(text):\n",
    "    # num of words requirement\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) < 50 or len(words) > 1e5:\n",
    "        return False\n",
    "\n",
    "    # word length requirement\n",
    "    word_lens = [len(word) for word in words]\n",
    "    mean_len = sum(word_lens)/len(words)\n",
    "    if mean_len < 3.0 or mean_len > 10.0:\n",
    "        return False\n",
    "\n",
    "    # at least one alphabetic character\n",
    "    n_valid_words = sum(1 for word in words if re.search(r'[a-zA-Z]', word))\n",
    "    if n_valid_words / len(words) < 0.8:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def valid_lines(text):\n",
    "    lines = re.split(r\"\\n+\", text)    \n",
    "    n_valid_lines = sum(1 for line in lines if not line.endswith(\"...\"))\n",
    "    if n_valid_lines / len(lines) > 0.7:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def gopher_quality_filter(text):\n",
    "    return valid_words(text) and valid_lines(text)\n",
    "\n",
    "\n",
    "text = \"This should definitely be a valid input text and of high quality according to Gopher rules.\\n\" *10\n",
    "gopher_quality_filter(text)\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fbea20",
   "metadata": {},
   "source": [
    "## Quality Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedcbfc",
   "metadata": {},
   "source": [
    "- To get positive texts; the filtering might be a bit cheating...\n",
    "    - `cat enwiki-20240420-extracted_urls.txt | grep \"https://en.wikipedia.org/wiki\" > enwiki-20240420-extracted_urls_subset.txt`\n",
    "    - `wget --tries=2 --timeout=5 -i enwiki-20240420-extracted_urls_subset.txt --warc-file=subsampled_positive_urls -O /dev/null`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastwarc import ArchiveIterator\n",
    "from fastwarc.warc import WarcRecord\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from resiliparse.parse.encoding import detect_encoding\n",
    "\n",
    "def extract_text(record):\n",
    "    byte_string = record.reader.read()\n",
    "    encoding = detect_encoding(byte_string)\n",
    "    try:\n",
    "        html_content = byte_string.decode(encoding=encoding)\n",
    "    except UnicodeDecodeError:\n",
    "        return \"\"\n",
    "    extracted_text = extract_plain_text(html_content)\n",
    "    return extracted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0813af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_html_record(record: WarcRecord) -> bool:\n",
    "    try:\n",
    "        return record.http_headers.get('Content-Type', '').startswith('text/html')\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "warc_file = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data/subsampled_positive_urls.warc.gz\"\n",
    "iterator = ArchiveIterator(open(warc_file, \"rb\"))\n",
    "\n",
    "from language_identification import identify_language\n",
    "positive_texts = []\n",
    "for record in iterator:\n",
    "    if is_html_record(record):\n",
    "        text = extract_text(record)\n",
    "        if len(text) > 512:  # very low bar for wikipedia articles\n",
    "            lang, score = identify_language(text)\n",
    "            if lang == \"en\" and score > 0.8:\n",
    "                positive_texts.append(text)\n",
    "\n",
    "print(f\"Raw number of positive texts: {len(positive_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc27014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from language_identification import identify_language\n",
    "\n",
    "def get_random_english_records_as_negative_texts(iterator, N=500, skip_prob=0.8):\n",
    "    \"\"\"Skip records randomly and return extracted text immediately\"\"\"\n",
    "    random_data = []\n",
    "    for record in iterator:\n",
    "        if len(random_data) >= N:\n",
    "            break\n",
    "        if random.random() > skip_prob:\n",
    "            # Read and process immediately while record is still fresh\n",
    "            extracted_text = extract_text(record)\n",
    "            lang, score = identify_language(extracted_text)\n",
    "            if lang == \"en\" and score > 0.9:\n",
    "                random_data.append(extracted_text)\n",
    "    return random_data\n",
    "\n",
    "warc_file = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data/CC-MAIN-20250417135010-20250417165010-00065.warc.gz\"\n",
    "iterator = ArchiveIterator(open(warc_file, \"rb\"), func_filter=lambda r: r.headers.get('WARC-Identified-Payload-Type') == 'text/html')\n",
    "negative_texts = get_random_english_records_as_negative_texts(iterator)\n",
    "\n",
    "print(f\"Raw number of negative texts: {len(negative_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "def chunk_text_simple(text, tokenizer, max_length=512, stride=64):\n",
    "    \"\"\"Simpler version using return_overflowing_tokens\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            'input_ids': tokens['input_ids'][i].tolist(),\n",
    "            'attention_mask': tokens['attention_mask'][i].tolist()\n",
    "        }\n",
    "        for i in range(len(tokens['input_ids']))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc674cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_chunks = [chunk_text_simple(text, tokenizer) for text in positive_texts]\n",
    "positive_chunks = [chunk for chunks in positive_chunks for chunk in chunks]\n",
    "print(len(positive_chunks))\n",
    "\n",
    "negative_chunks = [chunk_text_simple(text, tokenizer) for text in negative_texts]\n",
    "negative_chunks = [chunk for chunks in negative_chunks for chunk in chunks]\n",
    "print(len(negative_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle chunks before splitting\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "random.shuffle(negative_chunks)\n",
    "random.shuffle(positive_chunks)\n",
    "\n",
    "# Create train/valid/test splits\n",
    "n_neg = len(negative_chunks)\n",
    "n_pos = len(positive_chunks)\n",
    "\n",
    "print(f\"Total negative chunks: {n_neg}\")\n",
    "print(f\"Total positive chunks: {n_pos}\")\n",
    "\n",
    "# Split indices\n",
    "train_neg = negative_chunks[:1000]\n",
    "valid_neg = negative_chunks[1000:1200]\n",
    "\n",
    "train_pos = positive_chunks[:1000]\n",
    "valid_pos = positive_chunks[1000:1200]\n",
    "\n",
    "# Create datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "train_chunks = train_neg + train_pos\n",
    "train_labels = [0] * len(train_neg) + [1] * len(train_pos)\n",
    "\n",
    "valid_chunks = valid_neg + valid_pos\n",
    "valid_labels = [0] * len(valid_neg) + [1] * len(valid_pos)\n",
    "\n",
    "ds_train = Dataset.from_dict({\n",
    "    \"input_ids\": [chunk['input_ids'] for chunk in train_chunks],\n",
    "    \"attention_mask\": [chunk['attention_mask'] for chunk in train_chunks],\n",
    "    \"label\": train_labels\n",
    "})\n",
    "\n",
    "ds_valid = Dataset.from_dict({\n",
    "    \"input_ids\": [chunk['input_ids'] for chunk in valid_chunks],\n",
    "    \"attention_mask\": [chunk['attention_mask'] for chunk in valid_chunks],\n",
    "    \"label\": valid_labels\n",
    "})\n",
    "\n",
    "print(f\"\\nTrain: {len(ds_train)} samples (neg: {sum(1 for l in train_labels if l == 0)}, pos: {sum(1 for l in train_labels if l == 1)})\")\n",
    "print(f\"Valid: {len(ds_valid)} samples (neg: {sum(1 for l in valid_labels if l == 0)}, pos: {sum(1 for l in valid_labels if l == 1)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"quality_classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11371a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "with open(\"/home/azureuser/localfiles/cs336-assignment4-data-mine/tests/fixtures/high_quality_wiki_reference.txt\") as f:\n",
    "    hq_text = f.read()\n",
    "with open(\"/home/azureuser/localfiles/cs336-assignment4-data-mine/tests/fixtures/low_quality_cc.txt\") as f:\n",
    "    lq_text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0973a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hq_chunks = chunk_text_simple(hq_text, tokenizer)\n",
    "lq_chunks = chunk_text_simple(lq_text, tokenizer)\n",
    "# positive_chunks = [chunk for chunks in positive_chunks for chunk in chunks]\n",
    "# print(len(positive_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on lq_text\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "predictions = []  # Fresh list each time\n",
    "\n",
    "with torch.no_grad():\n",
    "    for chunk in hq_chunks:\n",
    "        # Convert to tensors and add batch dimension\n",
    "        inputs = {\n",
    "            'input_ids': torch.tensor([chunk['input_ids']]).cuda(),\n",
    "            'attention_mask': torch.tensor([chunk['attention_mask']]).cuda()\n",
    "        }\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        predictions.append({\n",
    "            'label': torch.argmax(probs, dim=-1).item(),\n",
    "            'prob_negative': probs[0][0].item(),\n",
    "            'prob_positive': probs[0][1].item()\n",
    "        })\n",
    "\n",
    "# Count predictions\n",
    "positive_count = sum(1 for p in predictions if p['label'] == 1)\n",
    "negative_count = sum(1 for p in predictions if p['label'] == 0)\n",
    "total_chunks = len(predictions)\n",
    "\n",
    "positive_ratio = positive_count / total_chunks\n",
    "negative_ratio = negative_count / total_chunks\n",
    "\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "print(f\"Positive chunks: {positive_count} ({positive_ratio:.2%})\")\n",
    "print(f\"Negative chunks: {negative_count} ({negative_ratio:.2%})\")\n",
    "print(f\"\\nFinal prediction: {'POSITIVE (high quality)' if positive_ratio > 0.5 else 'NEGATIVE (low quality)'}\")\n",
    "\n",
    "print(f\"\\nPer-chunk predictions:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Chunk {i+1}: {id2label[pred['label']]} (pos: {pred['prob_positive']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a5e9f",
   "metadata": {},
   "source": [
    "## Deduplication\n",
    "### Exact line dedpulication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5ef86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/tests/fixtures/documents_with_line_duplicates\"\n",
    "dir_tgt = \"/home/azureuser/localfiles/cs336-assignment4-data-mine/cs336_data\"\n",
    "files = [os.path.join(dir, f) for f in os.listdir(dir)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f047aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fd4c1f199cde9a998612f5de0a292e29': {'line_id': [0], 'file_basenames': ['doc5.txt']}, 'a48331fd9a20853375a5f9008858091f': {'line_id': [0, 0], 'file_basenames': ['doc4.txt', 'doc3.txt']}, '6548b245bba14aff59a41ea60098e88d': {'line_id': [1, 1], 'file_basenames': ['doc4.txt', 'doc3.txt']}, 'f9ff8a2ee66a24cc2a715a2d345a7a1c': {'line_id': [2], 'file_basenames': ['doc4.txt']}, '9c6c6c00ff2c041741c41fcee56d6846': {'line_id': [0, 0], 'file_basenames': ['doc2.txt', 'doc1.txt']}, 'a4ac8aeedce3b7e9f2ac1b07527aaa56': {'line_id': [2], 'file_basenames': ['doc3.txt']}, 'b9013db2f2b976d2a223fda39a8fab4b': {'line_id': [3], 'file_basenames': ['doc3.txt']}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import hashlib\n",
    "\n",
    "line_dict = {}\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    lines = re.split(r'\\n+', content)\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        if not line.strip():  # skip empty lines\n",
    "            continue\n",
    "        hash = hashlib.md5(line.encode()).hexdigest()\n",
    "        file_basename = os.path.basename(file)\n",
    "        # if hash not in line_dict:\n",
    "        #     line_dict[hash] = [file_name]\n",
    "        # else:\n",
    "        #     if file_name not in line_dict[hash]:\n",
    "        #         line_dict[hash].append(file_name)\n",
    "        if hash not in line_dict:\n",
    "            line_dict[hash] = {\n",
    "                \"line_id\": [i],\n",
    "                \"file_basenames\": [file_basename]\n",
    "            }\n",
    "        else:\n",
    "            line_dict[hash][\"line_id\"].append(i)\n",
    "            line_dict[hash][\"file_basenames\"].append(file_basename)\n",
    "\n",
    "print(line_dict)\n",
    "# line_dict = {k:list(set(v)) for k, v in line_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80c8db26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc4.txt': [0, 1], 'doc3.txt': [0, 1], 'doc2.txt': [0], 'doc1.txt': [0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dict = {}\n",
    "for k, v in line_dict.items():\n",
    "    line_ids, file_basenames = v.values()\n",
    "    if len(set(file_basenames)) == 1:\n",
    "        continue\n",
    "    for line_id, file_basename in zip(line_ids, file_basenames):\n",
    "        if file_basename not in file_dict:\n",
    "            file_dict[file_basename] = [line_id]\n",
    "        else:\n",
    "            file_dict[file_basename].append(line_id)\n",
    "\n",
    "file_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "973485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for file in files:\n",
    "    file_basename = os.path.basename(file)\n",
    "    file_tgt = os.path.join(dir_tgt, file_basename)\n",
    "    if file_basename not in file_dict:\n",
    "        shutil.copy(file, file_tgt)\n",
    "        continue\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    lines = re.split(r'\\n+', content)\n",
    "    lines2rm_ids = file_dict[file_basename]\n",
    "\n",
    "    new_lines = [lines[i] for i in range(len(lines)) if i not in lines2rm_ids]\n",
    "    with open(file_tgt, 'w') as f:\n",
    "        f.write(\"\\n\".join(new_lines))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
